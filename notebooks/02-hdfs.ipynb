{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left\"  src=\"images/hdfs.png\">\n",
    "<img style=\"float: right\" src=\"images/surfsara.png\">\n",
    "\n",
    "<hr style=\"clear: both\" />\n",
    "\n",
    "# HDFS introduction\n",
    "\n",
    "Below are number of exercises in Python and some in Shell. Press Shift-Enter to execute the code. You can use code completion by using tab.\n",
    "\n",
    "In this notebook we will start with some HDFS basics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS at SURFsara\n",
    "\n",
    "The Hadoop cluster at SURFsara is configured as HDFS cluster. It currently consists of 198 machines each offering part of their local disk space to the distributed file system. The configured capacity is 2.26 PB. Taking default replication into account there is room for 753 TB of data. The system hosts around 28 million files and directories for various users.\n",
    "\n",
    "To accomodate many users the Hadoop cluster at SURFsara is secured by [Kerberos](https://en.wikipedia.org/wiki/Kerberos_(protocol). In order to make use of any cluster services we will first need to authenticate. The notebook environment is preconfigured with credentials that we only need to initialize. Execute the next cell to do this. The exclamation mark in the cell instructs Jupyter to execute that line as a shell command instead of Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "! kinit.sh\n",
    "cluster_user = !klist | egrep -o 'hadws[0-9]+'\n",
    "cluster_user = str(cluster_user[0])\n",
    "print \"My user name on the cluster is %s\" % cluster_user\n",
    "os.environ['CLUSTER_USER'] = cluster_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well you should see some output listing your configured user (note that it should match the first part of the notebook URL). This username has been assigned to the `cluster_user` variable accessible from this notebook and to the `$CLUSTER_USER` shell environment variable.\n",
    "\n",
    "## Basic HDFS operations\n",
    "\n",
    "Snakebite is a Python library that provides a pure Python HDFS client. The client uses protobuf for communicating with the Namenode and comes in the form of a library and a command line interface. Currently, the snakebite client supports most actions that involve the Namenode and reading data from Datanodes. Note that writing data is currently not supported. Writing can be achieved by using either the HDFS Java API or hdfs shell commands. The former is beyond the scope of this tutorial the latter will be used in exercises in this notebook.\n",
    "\n",
    "First initialize a client for HDFS. For more detail concerning the snakebite API please see the [snakebite documentation](http://snakebite.readthedocs.org/en/latest/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snakebite.client import AutoConfigClient\n",
    "client = AutoConfigClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The client object exposes various methods. Let's start with a listing of directory contents using the `ls` function. The function takes as argument a path on HDFS. We will list a public data directory: `/data/public/hadws`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "for i in client.ls([\"/data/public/hadws\"]):\n",
    "    pp.pprint(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "snakebite returns the result of the listing as an array of dicts. Note that many properties that are available on regular file systems, such as size, path, owner and permissions are present on HDFS as well. Something that is not very common though is the `block_replication` factor. This factor denotes how many times the file is present on the cluster  file system. List the path and the replication factor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in client.ls([\"/data/public/hadws\"]):\n",
    "    print(i[\"path\"] + \" \" +  str(i[\"block_replication\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's proceed to read a file. Both snakebite and the HDFS command line support a text operation where data is read from HDFS and converted to text (note that this is not always possible for all data formats). Use the text method to print alice.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = client.text([\"/data/public/hadws/alice.txt\"])\n",
    "for i in text:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `hdfs` command-line program for this is very similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -text /data/public/hadws/alice.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `/data/public/hadws/`directory contains the data files that will be used in subsequent notebooks. As an exercise you are required to copy them to the `/user/$CLUSTER_USER` directory. Snakebite does not offer any copy method so we will do this using the [`hdfs dfs`](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html#cp) shell commands. Note that the reference manual uses `hadoop fs` this is a synonym for `hdfs dfs`. Please use the latter to copy all data files to the directory of your `$CLUSTER_USER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -ls /user/\"$CLUSTER_USER\"\n",
    "! hdfs dfs -cp /data/public/hadws/* /user/\"$CLUSTER_USER\"\n",
    "! hdfs dfs -ls /user/\"$CLUSTER_USER\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next make a subdirectory in `/user/$CLUSTER_USER` named tmp. Copy alice.txt to this subdirectory and rename it to wonderland.txt and recursively list the  `/user/$CLUSTER_USER` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -mkdir /user/\"$CLUSTER_USER\"/tmp\n",
    "! hdfs dfs -cp /user/\"$CLUSTER_USER\"/alice.txt /user/\"$CLUSTER_USER\"/tmp/wonderland.txt\n",
    "! hdfs dfs -ls -R /user/$CLUSTER_USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that HDFS stores your files in blocks that are replicated across the cluster. There is a command that can show you information about the physical location of those blocks. Execute the next cell to see where the blocks for the `/user/$CLUSTER_USER/tmp/wonderland.txt` are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs fsck /user/\"$CLUSTER_USER\"/tmp/wonderland.txt -files -blocks -locations -racks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should show a list of the IP addresses and rack names of the machines the block is located on (e.g. /S43/145.100.41.61:1004). Next increase the replication factor to 10 of the `/user/$CLUSTER_USER/tmp/wonderland.txt` file by using the `hdfs dfs -setrep` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -setrep 10 /user/\"$CLUSTER_USER\"/tmp/wonderland.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about what happened when you increased the replication before using `hdfs fsck` again to check the results. What effects will increasing the replication have for processing the data and fault tolerance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs fsck /user/\"$CLUSTER_USER\"/tmp/wonderland.txt -files -blocks -locations -racks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was the output what you expected? Chances are that the system did not manage to create all replica's yet and you will see a message about under replicated blocks. HDFS block operations are not executed with high priority, this minimizes peaks in overall network usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, download the `/user/$CLUSTER_USER/tmp/wonderland.txt` to the machine hosting the notebook, list the current directory of the notebook environment and delete the `/user/$CLUSTER_USER/tmp/wonderland.txt` file and `/user/$CLUSTER_USER/tmp/` directory from HDFS. List your HDFS home at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -get /user/\"$CLUSTER_USER\"/tmp/wonderland.txt .\n",
    "! ls -lah\n",
    "! hdfs dfs -rm -R /user/\"$CLUSTER_USER\"/tmp\n",
    "! hdfs dfs -ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
