{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left\" src=\"images/spark.png\" />\n",
    "<img style=\"float: right\" src=\"images/surfsara.png\" />\n",
    "<hr style=\"clear: both\" />\n",
    "\n",
    "# Introduction to Apache Spark\n",
    "\n",
    "In this final notebook we will explore Spark using the Python API, called PySpark.\n",
    "\n",
    "_You can edit the cells below and execute the code by selecting the cell and press Shift-Enter. Code completion is supported by use of the Tab key._\n",
    "\n",
    "During the exercises you may want to refer to the [PySpark documentation](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html#pyspark.RDD) for more information on possible transformations and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize Spark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "if not 'sc' in globals():\n",
    "    conf = SparkConf().setMaster('local[*]')\n",
    "    sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a RDD from a list\n",
    "\n",
    "All parallel work in Spark is done on RDDs, so the first thing we need to do is convert our data (in this case a list) to an RDD. We will use the `parallelize` method on the `SparkContext sc`. It takes two arguments: (1) the collection, and (2) the number of partitions (splits). The second argument is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of words_list is: <type 'list'>\n",
      "the type of words_rdd is: <class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "words_list = ['Dog', 'Cat', 'Rabbit', 'Hare', 'Deer', 'Gull', 'Woodpecker', 'Mole']\n",
    "words_rdd = sc.parallelize(words_list, 2)\n",
    "\n",
    "print 'the type of words_list is: ' + str(type(words_list))\n",
    "print 'the type of words_rdd is: ' + str(type(words_rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map transformation\n",
    "\n",
    "We now want to change all words in the `words_rdd` to their plural form. We will do this using a [map](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=map#pyspark.RDD.map) transformation.\n",
    "Remember that the map function will apply the function to each element of the RDD. \n",
    "\n",
    "First, we will write a simple function that takes a single word as argument and return the word with an 's' added to it. In the next step we will use this function in a map transformation of the `words_rdd`.\n",
    "\n",
    "Take a look at the function below and fill in the code at the tag &lt;FILL IN&gt;."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def makePlural(word):\n",
    "    \"\"\"Adds an 's' to `word`.\n",
    "\n",
    "    Note:\n",
    "        This is a simple function that only adds an 's'.  \n",
    "\n",
    "    Args:\n",
    "        word (str): A string.\n",
    "\n",
    "    Returns:\n",
    "        str: A string with 's' added to it.\n",
    "    \"\"\"\n",
    "    return <FILL IN>\n",
    "\n",
    "print makePlural('cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the `makePlural` function as input for the map transformation on `words_rdd`.\n",
    "The action [`collect`](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=collect#pyspark.RDD.collect) transfers the content of the RDD to the driver. Note, that a large RDD may be scattered over many machines. In such a case a `collect` can be a very bad idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "plural_rdd = plural_rdd.map<FILL IN>\n",
    "print plural_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using lambda functions\n",
    "\n",
    "But wait a minute! We can actually achieve the same functionality by using lambda functions. In this case we define makePlural as a lambda function. In this case we define `makePlural` as a lambda function. \n",
    "\n",
    "Hint: The map function needs a function as argument. This function needs one argument, let's call that `x`. The body of the function adds an 's' to the end of `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# A lambda function for adding s at the end of a string\n",
    "lambda_plural_rdd = words_rdd.map(<FILL IN>)\n",
    "\n",
    "print lambda_plural_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do another map transformation. For each word in `words_rdd` determine its length. The Python function `len`  will return the length of a string.\n",
    "\n",
    "You can do this with a lambda function, but there is another way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "word_lengths = (<FILL IN>\n",
    "                 .collect())\n",
    "print word_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your solution by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from test_helper import Test\n",
    "\n",
    "Test.assertEquals(sorted(word_lengths), [3, 3, 4, 4, 4, 4, 6, 10], 'incorrect value for word_lengths')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## RDD from a text file\n",
    " \n",
    "Manipulating a list with eight elements gets boring pretty fast, so let's move start processing a file. In this part we will read the file `alice.txt` from HDFS and will count the number of occurences of every word. This 'Word Count' is the ['Hello World'](https://en.wikipedia.org/wiki/%22Hello,_World!%22_program) of data-processing frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# authenticate for access to the HDFS\n",
    "!kinit.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an RDD from alice.txt where every element is a line of the file\n",
    "alice_rdd = sc.textFile('alice.txt').cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `collect` call not accepted\n",
    "\n",
    "As we mentioned before, once your data no longer fits on the screen the `collect` method becomes less useful or even problematic. But we still want to have a way to inspect the (intermediate) results. For this we can use one of the following methods as a replacement of `collect`:\n",
    "\n",
    "- `first()`: return the first elements of the rdd \n",
    "- `take(n)`:  return a list of `n` elements\n",
    "- `takeOrdered(n, [key=f])`: return the first n elements of the rdd, the order is defined by the optional function f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print alice_rdd.first()\n",
    "\n",
    "print 'first: ' + alice_rdd.first()\n",
    "print 'take(5): ' + str(alice_rdd.take(5))\n",
    "print 'takeOrdered(5, key=lambda x: -x)' + \n",
    "       str(alice_rdd.takeOrdered(5, key=lambda x: -x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The one-to-many map: flatMap\n",
    "\n",
    "We have an RDD of lines, so let's try to convert this to an RDD of words by splitting the lines on whitespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alice_words_try = alice_rdd.map(lambda line: line.split())\n",
    "\n",
    "print alice_words_try.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look right! We want an RDD of words, but we have created an RDD of lists of words.\n",
    "\n",
    "We want to map a function on an input that returns multiple values in a list, but then not to want the output nested in the same way as the input was. As it is commonly needed Spark includes a [`flatMap`](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=map#pyspark.RDD.flatMap) transformation that will _flatten_ the output of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alice_words = alice_rdd.flatMap(lambda line: line.split())\n",
    "\n",
    "print alice_words.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key-Value Pairs\n",
    "\n",
    "In order to count words in parallel we are going to use an RDD which consist of simple key-value pairs. We will call this RDD `alice_pairs` and it will be result of a transformation of `alice_words`. For every word in `alice_words` we want to have a `(word, 1)` tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "alice_pairs = <FILL IN>\n",
    "\n",
    "print alice_pairs.takeOrdered(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test.assertEquals(set(alice_pairs.takeOrdered(10),\n",
    "                      {}\n",
    "                 'incorrect value for alice_pairs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReduceByKey\n",
    "\n",
    "Next, we are going to count all words by using [`reduceByKey`](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=reducebykey#pyspark.RDD.reduceByKey).\n",
    "\n",
    "`ReducebyKey` expects the RDD to consist of key-value pairs an it will perform a reduce operation per key. \n",
    "It will need a two-argument function as input that will work on the values only. Remember that a reduce function needs two arguments and will reduce all elements to a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Note that reduceByKey takes in a function that accepts two values and returns a single value\n",
    "# The function that is input to reduceByKey only works on the values. Spark will execute this function per key\n",
    "\n",
    "word_counts = word_pairs.reduceByKey(lambda x,y : <FILL IN>)\n",
    "\n",
    "top10_words = wordCounts.takeOrdered(10, key=lambda p: -p[1])\n",
    "\n",
    "print top10_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Counting using reduceByKey\n",
    "Test.assertEquals(set(top10_words),\n",
    "                  {},\n",
    "                  'incorrect value for word_counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReduceByKey - background\n",
    "\n",
    "The `reduceByKey` method is similar to Hadoop's Reduce, but more restrictive. The function you provide to `reduceByKey` needs to be both [commutative][https://en.wikipedia.org/wiki/Commutative_property] and [associative][https://en.wikipedia.org/wiki/Associative_Property].\n",
    "\n",
    "These restrictions allow Spark to perform additional optimisations, performing the operation on each partitions of the RDD and minimising network traffic. In the example below at most six values need to be transmitted.\n",
    "\n",
    "![reduceByKey](images/reducebykey.png)\n",
    "(Picture by DataBricks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tweets Analysis\n",
    "\n",
    "As an example of how to analyse a file, we will look at a file with Dutch tweets. We will read in the file by making use of [sc.textFile](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=textfile#pyspark.SparkContext.textFile).\n",
    "\n",
    "We will be using 4 partitions here. Take a look at the line where we filter and then map the data to utf-8 encoding. Note the way transformations are chained together.\n",
    "\n",
    "Print out the first tweet in the RDD by making use of the [take](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=take#pyspark.RDD.take) action. It needs as argument the number of elements in the RDD that it will send to the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Use take to print out the first tweet\n",
    "\n",
    "tweetRDD = (sc.textFile('data/tweets.txt', 4)\n",
    "            .filter(lambda x : len(x) > 0)\n",
    "            .map(lambda x: x.encode('utf-8')))\n",
    "\n",
    "print tweetRDD.<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Conversion to json**\n",
    "\n",
    "Next, we are going to convert the tweets into JSON format. This will return a dictionary where each key is an attribute of the key. Some attributes, like *user* have sub-attributes.\n",
    "\n",
    "In the next cell the conversion takes place and the first tweet is shown. \n",
    "\n",
    "Just execute the cell, there's noting to fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "jsonTweetRDD = tweetRDD.map(lambda x: json.loads(x))\n",
    "parsed = jsonTweetRDD.take(1)\n",
    "print json.dumps(parsed, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Access to fields in the tweets**\n",
    "\n",
    "We have made some selections for you to show how to access fields in tweets.\n",
    "\n",
    "This is pure Python, although the data is contained in an RDD. You probably see what's going on here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsonTweetRDDtext = jsonTweetRDD.map(lambda x: [x['text'],\n",
    "                                               x['created_at'],\n",
    "                                               x['entities']['hashtags'],\n",
    "                                               x['user']['name'], \n",
    "                                               x['user']['screen_name'], \n",
    "                                               x['user']['followers_count'], \n",
    "                                               x['user']['description']])\n",
    "print jsonTweetRDDtext.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**RDDs are distributed over workers**\n",
    "\n",
    "It is important to understand what code is executed on workers, and what code on the driver. To move data to and from the driver to the workers is very expensive.\n",
    "\n",
    "RDDs are distributed over workers and transformations define a sequence of RDDs. Never try to define an RDD inside an RDD and beware of what code is executed by the driver.\n",
    "\n",
    "Let's make a quick list of all attributes in a tweet. We'll do it the wrong way first, by doing a map on the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print jsonTweetRDD.map(lambda x: x.keys()).take(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Another attempt**\n",
    "\n",
    "The previous code is very inefficient, since all tweets in the RDD are processed, and we end up with an RDD with all keys for all tweets. It would be better to take a single tweet and then outside an RDD compute the keys. Note that then the computation of the keys is done by the driver.\n",
    "\n",
    "Try to do this in a single statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "print jsonTweetRDD.take<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Selecting Text**##\n",
    "\n",
    "We select only the text from the tweets and clean it up a bit. Then we [cache](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=cache#pyspark.RDD.cache) the new RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "tweetTextRDD = (jsonTweetRDD.map(lambda x: x['text'])\n",
    "                            .map(lambda x: x.encode('utf-8'))\n",
    "                            .map(lambda x: re.sub(r'[^\\w]',' ', x))                   \n",
    "                            .cache()\n",
    "                            )\n",
    "print tweetTextRDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Filtering**##\n",
    "\n",
    "Use the [filter](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=filter#pyspark.RDD.filter) transformation to select only tweets that contain the word 'ik'.\n",
    "Filter takes a boolean function as argument and returns those elements of the RDD which are true in respect to this function.\n",
    "\n",
    "Make sure to convert the words in the tweets to lower case, before filtering.\n",
    "Then count the words using [count](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=count#pyspark.RDD.count) and print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "ikRDD = tweetTextRDD.<FILL IN>\n",
    "count = <FILL IN>\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Test.assertEquals(count, 221, 'Wrong count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Counting words in tweets**\n",
    "Next count the words in tweets by applying four transformations in a chain on tweetTextRDD.\n",
    "\n",
    "Note that tweetTextRDD is an RDD which contains lines (strings).\n",
    "\n",
    "First, use string split on a single white space for every line in the RDD. Instead of Map, use [flatMap](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=flatmap#pyspark.RDD.flatMap) (Why?)<br>\n",
    "Second, we only want to see words with a length larger than 1.<br> \n",
    "Third, use a map transformation to convert each word to lower case and create a (word, 1) tuple.<br>\n",
    "Finally, use reduceByKey to add the result for each word.<br>\n",
    "\n",
    "We will print the result by making use of the [takeOrdered](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=takeordered#pyspark.RDD.takeOrdered.) action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Use flatmap, filter, map and reduceByKey, to split the lines, filter words smaller that 2 characters, \n",
    "# create (word, 1) tuples and add up the results\n",
    "aRDD = (tweetTextRDD.<FILL IN>\n",
    "        .<FILL IN>\n",
    "        .<FILL IN>\n",
    "        .<FILL IN>\n",
    "print aRDD.takeOrdered(35, lambda x : -x[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
