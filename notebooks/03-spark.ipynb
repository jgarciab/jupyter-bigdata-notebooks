{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/spark.png)\n",
    "<img style=\"float: right\" src=\"images/surfsara.png\">\n",
    "## **Introduction to Apache Spark**\n",
    "\n",
    "Below are number of exercises in PySPark. Press Shift-Enter to execute the code. You can use code completion by using tab.\n",
    "\n",
    "During the exercises you may want to refer to [The PySpark documentation](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html#pyspark.RDD) for more information on possible transformations and actions.\n",
    "\n",
    "Let us first create a simple RDD, based on a list of words. We will be using two partitions here for the RDD. We will use a SparkContext sc that has already been created for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "wordsList = ['Dog', 'Cat', 'Rabbit', 'Hare', 'Deer', 'Gull', 'Woodpecker', 'Mole']\n",
    "wordsRDD = sc.parallelize(wordsList, 2)\n",
    "# Print out the type of wordsRDD\n",
    "print type(wordsRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Map transformation **\n",
    "\n",
    "We now want to change all words in the wordsRDD to their plural form. We will do this using a [map](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=map#pyspark.RDD.map) transformation.\n",
    "Remember that the map function will apply the function to each element of the RDD. \n",
    "\n",
    "First, we will write a simple function that takes a single word as argument and return the word with an 's' added to it. In the next step we will use this function in a map transformation of the wordsRDD.\n",
    "\n",
    "Take a look at the function below and fill in the code at the tag <FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def makePlural(word):\n",
    "    \"\"\"Adds an 's' to `word`.\n",
    "\n",
    "    Note:\n",
    "        This is a simple function that only adds an 's'.  \n",
    "\n",
    "    Args:\n",
    "        word (str): A string.\n",
    "\n",
    "    Returns:\n",
    "        str: A string with 's' added to it.\n",
    "    \"\"\"\n",
    "    return <FILL IN>\n",
    "\n",
    "print makePlural('cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the makePlural function as input for the map transformation on wordsRDD.\n",
    "The action [collect()](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=collect#pyspark.RDD.collect) transfers the content of the RDD to the driver. Note, that a large RDD may be scattered over many machines. In such a case a collect may not be a good idea. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "pluralRDD = wordsRDD.map<FILL IN>\n",
    "print pluralRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Using lambda functions**\n",
    "\n",
    "We can achieve the same functionality by using lambda functions. In this case we define makePlural as a lambda function. \n",
    "\n",
    "Hint: The map function needs a lambda function as argument. This function needs one argument, let's call that x. The body of the function adds an 's' to the end of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A lambda function for adding s at the end of a string\n",
    "lambdaPluralRDD = wordsRDD.map(lambda x : x + 's')\n",
    "print lambdaPluralRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do another map transformation. For each word in wordRDD determine its length. The Python function len(s) will return the length for a string s.\n",
    "\n",
    "You can do this with a lambda function, but there is another way... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "wordLengths = (<FILL IN>\n",
    "                 .collect())\n",
    "print wordLengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your solution by running the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Length of each word \n",
    "from test_helper import Test\n",
    "   \n",
    "Test.assertEquals(wordLengths, [3, 3, 6, 4, 4, 4, 10, 4],\n",
    "                  'incorrect values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Key Value Pairs**\n",
    "In order to count words in parallel we are going to use an RDD which consist of simple key value pairs. We will call this RDD wordPairs and it will be result of a transformation of wordsRDD. For every word wordsRDD we want to have a (word, 1) tuple. Please fill in the code in the next cell at the place indicated and run the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "wordPairs = <FILL IN>\n",
    "print wordPairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Pair RDDs (1f)\n",
    "Test.assertEquals(wordPairs.collect(),\n",
    "                  [('Dog', 1), ('Cat', 1), ('Rabbit', 1), ('Hare', 1), ('Deer', 1), ('Gull', 1), ('Woodpecker', 1), ('Mole', 1)],\n",
    "                  'incorrect value for wordPairs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**reduceByKey**\n",
    "\n",
    "Next, we are going to count all words by using [reduceByKey](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=reducebykey#pyspark.RDD.reduceByKey).\n",
    "\n",
    "reducebyKey expects the RDD to consist of key value pairs an it will perform a reduce operation per key. \n",
    "It will need a two-argument function as input that will work on the values only. Remember that a reduce function needs two arguments and will reduce all elements of the RDD to a single value.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Note that reduceByKey takes in a function that accepts two values and returns a single value\n",
    "# The function that is input to reduceByKey only works on the values. Spark will execute this function per key\n",
    "\n",
    "#from operator import add\n",
    "\n",
    "wordCounts = wordPairs.reduceByKey(lambda x,y : <FILL IN>)\n",
    "print wordCounts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Counting using reduceByKey (2c)\n",
    "Test.assertEquals(wordCounts.collect(), [('this', 1), ('a', 1), ('message', 1), ('was', 2), ('bad', 1), ('His', 1), ('idea', 1)],\n",
    "                  'incorrect value for wordCounts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##**groupByKey**\n",
    "\n",
    "Another transformation on RDDs is [groupByKey](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=reducebykey#pyspark.RDD.groupByKey)\n",
    "\n",
    "groupByKey works on key value pairs, tuples in Python. It groups all values with the same key together in a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc = wordPairsRDD.groupByKey()\n",
    "\n",
    "# The take action allows us to get the first n records from the RDD, in contrast with collect() which returns the \n",
    "# complete contents of the RDD\n",
    "# Here we take 3 records to print out, which is in this case happens to be the complete RDD...\n",
    "\n",
    "print wc.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When printing out these records we see tuples, with readable keys 'one', 'two', 'three', followed by ResultIterable objects, which Python does not not how to print. These objects are lists containing the values. We can print their contents by converting them to proper Python lists. To convert a ResultIterable y to a list we can simply use list(y).\n",
    "\n",
    "Let's think how to do this. The RDD is list of tuples (x,y), where y is the ResultIterable which we want to convert.\n",
    "A (lambda) function to convert one record would then take as input (x,y) and return (x, list(y))\n",
    "We then use map to to do this for the entire RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# Provide a lambda function to convert each ResultIterable object to a list\n",
    "\n",
    "wc1 = <FILL IN>\n",
    "print wc1.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen the result of groupBy is an RDD of which the records are tuples. Each tuple is a key and a list with values. We can get access to the elements the list by simply treating them as python lists. \n",
    "\n",
    "Let's print the first element of all these lists... which are all 1s of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print wc1.map(lambda (x,y): y[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**groupByKey vs reduceByKey **\n",
    "Here we will demonstrate the difference between [groupByKey](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=groupbykey#pyspark.RDD.groupByKey) (or group in general) and reducebyKey. Both can be used in similar ways and will, when applied correctly, lead to the same answers. However, the way they are computed by Spark is quite different. \n",
    "First, take a look at reduceByKey once more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This example will perform a word count by using reduceByKey\n",
    "\n",
    "words = [\"one\", \"two\", \"two\", \"three\", \"three\", \"three\"]\n",
    "# We create the RDD, immediately followed by a map - in a single statement\n",
    "# we could have done this in two steps as we have done above\n",
    "wordPairsRDD = sc.parallelize(words).map(lambda word: (word, 1))\n",
    "\n",
    "wordCountsWithReduce = (wordPairsRDD.reduceByKey(lambda x,y: x+y) \n",
    "                        .collect())\n",
    "print wordCountsWithReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test.assertEquals(sorted(wordCountsWithReduce), [('one', 1), ('three', 3), ('two', 2)], 'Error in word count with Reduce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** reduceByKey**\n",
    "The picture below shows how reduceByKey is computed on different workers. The reduceByKey function in the figure is equivalent to the one in the previous cell.\n",
    "\n",
    "![reduceByKey](https://dl.dropboxusercontent.com/u/7526640/reduce.png)\n",
    "(Picture by DataBricks)\n",
    "\n",
    "Now let's see how this is different from groupBy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc = wordPairsRDD.groupByKey()\n",
    "print wc.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Fill in a lambda function in groupBy\n",
    "# \n",
    "wordCountsWithGroup = (wordPairsRDD\n",
    "                       .groupByKey()\n",
    "                       <FILL IN>\n",
    "                       .collect())\n",
    "        \n",
    "print wordCountsWithGroup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test.assertEquals(wordCountsWithReduce, [('one', 1), ('three', 3), ('two', 2)], 'Error in word count with Reduce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **groupBy**\n",
    "\n",
    "The figure below shows an example of [groupByKey](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=groupby#pyspark.RDD.groupByKey). Note that all key-value pairs are send to different workers. This leads to a lot of network traffic which will hamper performance. \n",
    "\n",
    "Also, similar to MapReduce Spark determines to which machine a pair should be send to, by calling a partitioning function on the key of the pair. Note, that if there are many keys, with each very few values, this approach scales badly.\n",
    "\n",
    "In general groupByKey should be avoided, particular when using large data sets. You can also look at [*foldByKey*](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=foldbykey#pyspark.RDD.foldByKey) and [*combineByKey*](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=combinebykey#pyspark.RDD.combineByKey) for alternatives.\n",
    "\n",
    "![groupByKey](images/groupby.png)\n",
    "(Picture by DataBricks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##** Optional: Tweets Analysis**\n",
    "\n",
    "As an example of how to analyse a file, we will look at a file with Dutch tweets. We will read in the file by making use of [sc.textFile](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=textfile#pyspark.SparkContext.textFile).\n",
    "\n",
    "We will be using 4 partitions here. Take a look at the line where we filter and then map the data to utf-8 encoding. Note the way transformations are chained together.\n",
    "\n",
    "Print out the first tweet in the RDD by making use of the [take](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=take#pyspark.RDD.take) action. It needs as argument the number of elements in the RDD that it will send to the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Use take to print out the first tweet\n",
    "\n",
    "tweetRDD = (sc.textFile('data/tweets.txt', 4)\n",
    "            .filter(lambda x : len(x) > 0)\n",
    "            .map(lambda x: x.encode('utf-8')))\n",
    "\n",
    "print tweetRDD.<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Conversion to json**\n",
    "\n",
    "Next, we are going to convert the tweets into JSON format. This will return a dictionary where each key is an attribute of the key. Some attributes, like *user* have sub-attributes.\n",
    "\n",
    "In the next cell the conversion takes place and the first tweet is shown. \n",
    "\n",
    "Just execute the cell, there's noting to fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "jsonTweetRDD = tweetRDD.map(lambda x: json.loads(x))\n",
    "parsed = jsonTweetRDD.take(1)\n",
    "print json.dumps(parsed, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Access to fields in the tweets**\n",
    "\n",
    "We have made some selections for you to show how to access fields in tweets.\n",
    "\n",
    "This is pure Python, although the data is contained in an RDD. You probably see what's going on here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsonTweetRDDtext = jsonTweetRDD.map(lambda x: [x['text'],\n",
    "                                               x['created_at'],\n",
    "                                               x['entities']['hashtags'],\n",
    "                                               x['user']['name'], \n",
    "                                               x['user']['screen_name'], \n",
    "                                               x['user']['followers_count'], \n",
    "                                               x['user']['description']])\n",
    "print jsonTweetRDDtext.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**RDDs are distributed over workers**\n",
    "\n",
    "It is important to understand what code is executed on workers, and what code on the driver. To move data to and from the driver to the workers is very expensive.\n",
    "\n",
    "RDDs are distributed over workers and transformations define a sequence of RDDs. Never try to define an RDD inside an RDD and beware of what code is executed by the driver.\n",
    "\n",
    "Let's make a quick list of all attributes in a tweet. We'll do it the wrong way first, by doing a map on the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print jsonTweetRDD.map(lambda x: x.keys()).take(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Another attempt**\n",
    "\n",
    "The previous code is very inefficient, since all tweets in the RDD are processed, and we end up with an RDD with all keys for all tweets. It would be better to take a single tweet and then outside an RDD compute the keys. Note that then the computation of the keys is done by the driver.\n",
    "\n",
    "Try to do this in a single statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "print jsonTweetRDD.take<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Selecting Text**##\n",
    "\n",
    "We select only the text from the tweets and clean it up a bit. Then we [cache](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=cache#pyspark.RDD.cache) the new RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "tweetTextRDD = (jsonTweetRDD.map(lambda x: x['text'])\n",
    "                            .map(lambda x: x.encode('utf-8'))\n",
    "                            .map(lambda x: re.sub(r'[^\\w]',' ', x))                   \n",
    "                            .cache()\n",
    "                            )\n",
    "print tweetTextRDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Filtering**##\n",
    "\n",
    "Use the [filter](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=filter#pyspark.RDD.filter) transformation to select only tweets that contain the word 'ik'.\n",
    "Filter takes a boolean function as argument and returns those elements of the RDD which are true in respect to this function.\n",
    "\n",
    "Make sure to convert the words in the tweets to lower case, before filtering.\n",
    "Then count the words using [count](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=count#pyspark.RDD.count) and print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "ikRDD = tweetTextRDD.<FILL IN>\n",
    "count = <FILL IN>\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Test.assertEquals(count, 221, 'Wrong count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Counting words in tweets**\n",
    "Next count the words in tweets by applying four transformations in a chain on tweetTextRDD.\n",
    "\n",
    "Note that tweetTextRDD is an RDD which contains lines (strings).\n",
    "\n",
    "First, use string split on a single white space for every line in the RDD. Instead of Map, use [flatMap](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=flatmap#pyspark.RDD.flatMap) (Why?)<br>\n",
    "Second, we only want to see words with a length larger than 1.<br> \n",
    "Third, use a map transformation to convert each word to lower case and create a (word, 1) tuple.<br>\n",
    "Finally, use reduceByKey to add the result for each word.<br>\n",
    "\n",
    "We will print the result by making use of the [takeOrdered](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html?highlight=takeordered#pyspark.RDD.takeOrdered.) action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Use flatmap, filter, map and reduceByKey, to split the lines, filter words smaller that 2 characters, \n",
    "# create (word, 1) tuples and add up the results\n",
    "aRDD = (tweetTextRDD.<FILL IN>\n",
    "        .<FILL IN>\n",
    "        .<FILL IN>\n",
    "        .<FILL IN>\n",
    "print aRDD.takeOrdered(35, lambda x : -x[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
