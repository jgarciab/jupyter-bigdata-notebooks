{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img style=\"float: left\" src=\"images/spark.png\" />\n",
    "<img style=\"float: right\" src=\"images/surfsara.png\" />\n",
    "<hr style=\"clear: both\" />\n",
    "\n",
    "## Spark Structured API\n",
    "\n",
    "In the previous notebook you have seen distributed processing using RDDs is done. In this notebook we will look at the Structured API. We will see how you can use DataFrames and SQL to do common data processing operations. By the end you should have a feeling on the strengths and weaknesses of these different approaches.\n",
    "\n",
    "The first difference is our Spark _entrypoint_. For RDDs this was the 'SparkContext' (usually named `sc`). For DataFrames we will use a 'SparkSession', which is more powerfull and easier to use. By convention we name our SparkSession 'spark'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a SparkSession, the 'DataFrame version' of the SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames from Python collections\n",
    "\n",
    "Just like we have seen with `sc.parallelize`, we can create a DataFrame from an existing Python collection. In addition to the collection itself we will also describe the structure of the data by naming the columns. Additionally, we could  specify the data types of the columns, but in this case we can let Spark infer this automatically.\n",
    "\n",
    "As an example we will look at the inventory of a hypothetical mobile phone shop. We create a DataFrame from a list of tuples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phoneStock = [\n",
    "    ('iPhone 6', 'Apple', 6, 549.00),\n",
    "    ('iPhone 6s', 'Apple', 5, 585.00),\n",
    "    ('iPhone 7', 'Apple', 11, 739.00),\n",
    "    ('Pixel', 'Google', 8, 859.00),\n",
    "    ('Pixel XL', 'Google', 2, 959.00),\n",
    "    ('Galaxy S7', 'Samsung', 10, 539.00),\n",
    "    ('Galaxy S6', 'Samsung', 5, 414.00),\n",
    "    ('Galaxy A5', 'Samsung', 7, 297.00),\n",
    "    ('Galaxy Note 7', 'Samsung', 0, 841.00)\n",
    "]\n",
    "\n",
    "columns = ['model', 'brand', 'stock', 'unit_price']\n",
    "\n",
    "phoneDF = spark.createDataFrame(phoneStock, columns)\n",
    "\n",
    "print('the type of phoneStock: ' + str(type(phoneStock)))\n",
    "print('the type of phoneDF: ' + str(type(phoneDF)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(It is also possible to create a Spark DataFrame from a [pandas](http://pandas.pydata.org/) DataFrame)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame inspection\n",
    "\n",
    "When developing an application it is very useful to check the stucture and content of the DataFrames you are creating. We can view the contents of the DataFrame records using the [show](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.show) method. (This is similar to the RDD `take` method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "phoneDF.<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two methods to look at the structure of a DataFrame, `describe` and `printSchema`. `describe` gives a new DataFrame with statistics about numerical colums. `printSchema` is especially useful with complicated nested structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phoneDF.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phoneDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data extraction\n",
    "\n",
    "Now that we have our data in a DataFrame, we want to use it to manipulate the data. Let's start by selecting subsets of the data: specific columns and/or rows.\n",
    "\n",
    "### Selecting columns\n",
    "\n",
    "Often we are not interested in all the columns of our data. DataFrames make it very easy to select only a subset by using the `select` method. Realise that we are not modifying the original DataFrame, but creating a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select only the model column\n",
    "modelDF = phoneDF.select('model')\n",
    "modelDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select both the brand and model columns\n",
    "bmDF = phoneDF.select('brand', 'model')\n",
    "bmDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Select the model and stock columns\n",
    "msDF = <FILL IN>\n",
    "msDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Columns specifications\n",
    "\n",
    "In the previous examples we have used strings to specify the columns we want to select. This is one valid format, but there are a few others that can be used as well. Sometimes the more complicated ones are required because the shorter versions are ambiguous for Sparks parser. For example, all these are equivalent:\n",
    "\n",
    "```\n",
    "bmDF = phoneDF.select('brand', 'model')\n",
    "bmDF = phoneDF.select(['brand', 'model'])\n",
    "bmDF = phoneDF.select(phoneDF['brand'], phoneDF['model'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering rows\n",
    "\n",
    "We can filter specific rows by using the DataFrame `filter` method. The column specifications are the same as with the select method, but note we need to add the DataFrame name in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select rows with phones from Google\n",
    "googleDF = phoneDF.filter(phoneDF['brand'] == 'Google')\n",
    "\n",
    "googleDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Select the rows with unit_price less than 550.00\n",
    "\n",
    "cheapDF = <FILL IN>\n",
    "cheapDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating data\n",
    "\n",
    "An important part of data processing is the ability of combining multiple records, like we did with `reduceByKey`. In the DataFrame API this is a two-step process:\n",
    "\n",
    "First you group the data using the `groupBy` method. `groupBy` can operate on one or multiple columns. It will not actually perform the grouping but create a reference to a `GroupedData` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groupedDF = phoneDF.groupBy('brand')\n",
    "print(type(groupedDF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data is grouped we can apply one of the standard aggregation functions on it. They are listed at the [GroupedData](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData) API documentation. These are: `min`, `max`, `mean`, `sum` and `count`. We can apply an aggregation to all columns or a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Minimum for all columns\n",
    "minDF = groupedDF.min()\n",
    "\n",
    "minDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Maximum for just the unit_price column\n",
    "maxDF = <FILL IN>\n",
    "\n",
    "maxDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can combine different aggregations per column using the `agg` method on a GroupedData instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sumDF = groupedDF.agg({'stock': 'sum', 'unit_price': 'mean'})\n",
    "\n",
    "sumDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to/from RDD\n",
    "\n",
    "Sometimes you want to do data manipulations which would be very easy with RDD operations, but complicated with the DataFrame API. Fortunately you can convert between DataFrames and RDDs of type 'Row'. Going from DataFrame to RDD is quite simple. Going back from RDD to DataFrame is more difficult because you need to re-apply the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phoneRDD = phoneDF.rdd\n",
    "pluralRDD = phoneRDD.map(lambda r: r.brand + 's')\n",
    "pluralRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading structured files/sources\n",
    "\n",
    "One of the advantages of DataFrames is the ability to read already structured data and automatically import the structure in Spark. Spark contains readers for a number of formats such as csv, json, parquet, orc, text and jdbc. There are also third-party readers/connectors for databases such as MongoDB and Cassandra.\n",
    "\n",
    "Here we read some json-formatted tweets. As you can see the complicated schema is inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweetDF = spark.read.format(\"json\").load('../data/tweets.json')\n",
    "tweetDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested data\n",
    "\n",
    "Nested fields can be selected by using a dot-notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select the user-name and text:\n",
    "tweetDF.select('user.name', 'text').show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL\n",
    "\n",
    "The SQL API aims to be ANSI-SQL SQL2003 and Hive-SQL compatible. The expressiveness is very similar to the DataFrame API. You can access the SQL API from the SparkSession by using `spark.sql`. Before you can query DataFrames using SQL you need to 'register' them as SQL tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DataFrame version\n",
    "resDF = phoneDF.filter(phoneDF['stock'] > 7).select('model')\n",
    "resDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SQL version\n",
    "\n",
    "# Register the phoneDF DataFrame within SQL as a table with name 'phones'\n",
    "phoneDF.createOrReplaceTempView('phones')\n",
    "\n",
    "# Perform the SQL query on the 'phones' table\n",
    "resDF = spark.sql('SELECT model FROM phones WHERE stock > 7')\n",
    "resDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining with other data sets\n",
    "\n",
    "Often you want to combine multiple dataset and combine them on a shared columns. In this example we create an extra table with information about the phone manufacturer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "companies = [\n",
    "    ('Google', 'USA', 1998, 'Sundar Pichai'),\n",
    "    ('Samsung', 'South Korea', 1938 ,'Oh-Hyun Kwon' ),\n",
    "    ('Apple', 'USA', 1976 ,'Tim Cook')\n",
    "]\n",
    "\n",
    "columns = ['company_name', 'hq_country', 'founding_year', 'ceo']\n",
    "\n",
    "companyDF = spark.createDataFrame(companies, columns)\n",
    "companyDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To join two DataFrames, we use the `join` method on one of the DataFrames. This method takes two arguments: (1) the other DataFrame, and (2) a join relation. Here we join the two DataFrames on the brand/company_name columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joinedDF = phoneDF.join(companyDF, phoneDF['brand'] == companyDF['company_name'])\n",
    "joinedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a more complicated query that combines multiple steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# All the models from USA companies with more than 7 items in stock\n",
    "result = (phoneDF.join(companyDF, phoneDF['brand'] == companyDF['company_name'])\n",
    "          .filter(companyDF['hq_country'] == 'USA')\n",
    "          .filter(phoneDF['stock'] > 7)\n",
    "          .select('model')\n",
    ")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: word count in DataFrames\n",
    "\n",
    "It is also possible to use DataFrames for less-structured data such as text. Here we show how you could do word count with DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split, trim\n",
    "\n",
    "(spark\n",
    "     .read.text('../data/shakespeare.txt')\n",
    "     .select(\n",
    "        explode(\n",
    "            split(\"value\", \"\\W+\")\n",
    "        ).alias(\"word\")\n",
    "    ).groupBy(\"word\")\n",
    "     .count()\n",
    " ).orderBy(\"count\", ascending=0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-defined functions\n",
    "\n",
    "In the previous example we used the built-in split function. It is also possible to define an use a custom user-defined function, or udf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "def my_tokenize(s):\n",
    "    s = s.lower()\n",
    "    words = s.split()\n",
    "    return words\n",
    "\n",
    "returnType = ArrayType(StringType())\n",
    "\n",
    "tokenize_udf = udf(my_tokenize, returnType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "(spark\n",
    "     .read.text('../data/shakespeare.txt')\n",
    "     .select(\n",
    "        explode(\n",
    "            tokenize_udf(\"value\")\n",
    "        ).alias(\"word\")\n",
    "    ).groupBy(\"word\")\n",
    "     .count()\n",
    " ).orderBy(\"count\", ascending=0).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
